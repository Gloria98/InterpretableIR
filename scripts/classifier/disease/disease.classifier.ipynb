{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/read.dataframe.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can get your own apiKey\n",
    "apiKey = \"Bearer \" + \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdSI6Imx4ZzphcGkiLCJzYyI6WyJrZzpyZWFkIiwiZXh0cmFjdGlvbjpyZWFkIl0sImFpIjoiYXBpOmM5NjUxNmRmLWYyZjYtMDRhNC1mYzVjLWQ5MmFjZGM0ZWZjMSIsInVpIjoidXNlcjpjZmYxMjM0MS1lN2FmLWEzMmUtNjM3YS0yNjFlMjRjZmVkZDAiLCJpYXQiOjE1ODc0MDM1MjF9.EsFfBoTNKl2TVgEwHF_qs8n2gkgwqJVNB0MiHLzM2P0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synonyms(conceptGraphId):\n",
    "    '''\n",
    "    Function to return the sy\n",
    "    Args:\n",
    "        A disease in lexigram ID\n",
    "    Returns:\n",
    "        A lit of disease synonyms or preferred terms\n",
    "    '''\n",
    "    \n",
    "    url = \"https://api.lexigram.io//v1/lexigraph/concepts/\" + conceptGraphId\n",
    "    r = requests.get(url, headers={'Authorization': apiKey})\n",
    "    response = json.loads(r.text) \n",
    "    synonyms = [] \n",
    "    synonyms.append(response['label'])\n",
    "    synonyms += response['synonyms']\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ancestors(conceptGraphId):\n",
    "    '''\n",
    "    Function to return a list of ancestors\n",
    "    Args:\n",
    "        conceptGraphId in the lexigram KB\n",
    "    Returns:\n",
    "        a list of ancestors\n",
    "    '''\n",
    "    \n",
    "    url = \"https://api.lexigram.io/v1/lexigraph/concepts/\" + conceptGraphId + \"/ancestors\"\n",
    "    r = requests.get(url, headers={'Authorization': apiKey})\n",
    "    response = json.loads(r.text)\n",
    "    ancestors_list = []\n",
    "    \n",
    "    for match in response['results']:\n",
    "        if match[\"types\"] == ['FINDINGS', 'PROBLEMS']:\n",
    "            ancestors_list.append(match[\"label\"])\n",
    "    \n",
    "    return ancestors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_descendants(conceptGraphId):\n",
    "    '''\n",
    "    Function to return a list of descendants\n",
    "    Args:\n",
    "        conceptGraphId in the lexigram KB\n",
    "    Returns:\n",
    "        a list of descendants\n",
    "    '''\n",
    "\n",
    "    url = \"https://api.lexigram.io/v1/lexigraph/concepts/\" + conceptGraphId + \"/descendants\"\n",
    "    r = requests.get(url, headers={'Authorization': apiKey})\n",
    "    response = json.loads(r.text)\n",
    "    descendants_list = []\n",
    "\n",
    "    for match in response['results']:\n",
    "        if match[\"types\"] == ['FINDINGS', 'PROBLEMS']:\n",
    "            descendants_list.append(match[\"label\"])\n",
    "    \n",
    "    return descendants_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download the synonyms, ancestors and decendants for each topic to save computation time\n",
    "def save_lexigram_output(year):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    query_topics = read_query_topics(year,\"disease\")\n",
    "    for k,v in query_topics.items():\n",
    "        \n",
    "        sub_results = {}\n",
    "        \n",
    "        lexigram_conceptID = find_lexigram_id(v)\n",
    "        if lexigram_conceptID!=0:\n",
    "            sub_results[\"synonyms\"] = find_synonyms(lexigram_conceptID)\n",
    "            sub_results[\"ancestors\"] = find_ancestors(lexigram_conceptID)\n",
    "            sub_results[\"descendants\"] = find_descendants(lexigram_conceptID)\n",
    "        else:\n",
    "            sub_results[\"synonyms\"] = []\n",
    "            sub_results[\"ancestors\"] = []\n",
    "            sub_results[\"descendants\"] = []\n",
    "        \n",
    "        results[k]=sub_results\n",
    "        \n",
    "    fp = str(year)+\".disease.expansion.json\"\n",
    "    with open(fp, 'w') as f:\n",
    "        json.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_lexigram_output(2017)\n",
    "# save_lexigram_output(2018)\n",
    "# save_lexigram_output(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(year):\n",
    "    \n",
    "    # read dataframe and parsing\n",
    "    df = read_dataframe(year, 'disease')\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # read query topics\n",
    "    query_topics=read_query_topics(year,\"disease\")\n",
    "    \n",
    "    # read expansion terms\n",
    "    with open(str(year)+'.disease.expansion.json','r') as f:\n",
    "        for line in f.readlines():\n",
    "            expansion_terms_dict = json.loads(line)\n",
    "    f.close()\n",
    "            \n",
    "    # read acronyms: a dict of <disease, acronyms>\n",
    "    acronyms_dict = dict()\n",
    "    with open(\"acronyms.json\",'r') as f:\n",
    "         for line in f.readlines():\n",
    "            acronyms_dict = json.loads(line)\n",
    "    f.close()\n",
    "\n",
    "    # corpus folder path\n",
    "    folder_path = \"../../../data/corpus/\"\n",
    "    \n",
    "    # save results\n",
    "    feature_table = pd.DataFrame(columns=[\"count_match_self\", \"count_match_ancestor\", \"count_match_descendant\",\n",
    "                                         \"topicid\",\"docid\",\"label\"])\n",
    "    \n",
    "    print(\"Parsing year {}\".format(year))\n",
    "    \n",
    "    for topic in set(df.trec_topic_number):\n",
    "        \n",
    "        disease = query_topics[topic]\n",
    "        expansion_terms=expansion_terms_dict[str(topic)]\n",
    "        synonyms = expansion_terms[\"synonyms\"]\n",
    "        ancestors = expansion_terms[\"ancestors\"]\n",
    "        descendants = expansion_terms[\"descendants\"]\n",
    "        if disease in acronyms_dict.keys():\n",
    "            acronyms = acronyms_dict[disease].split(\" \")\n",
    "        else:\n",
    "            acronyms = []\n",
    "        \n",
    "        \n",
    "        df_topic = df.loc[df.trec_topic_number == topic]\n",
    "        for index, rows in df_topic.iterrows():\n",
    "            \n",
    "            docid = str(rows[\"trec_doc_id\"])\n",
    "            content = []\n",
    "            with open (folder_path+docid+\".txt\", 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    content.append(line.strip())\n",
    "            f.close()\n",
    "            raw_text = \" \".join(content)\n",
    "            text = raw_text.lower()\n",
    "            \n",
    "            # 1) count disease itself (original term+synonyms+acronyms)\n",
    "            count_match_self = text.count(disease.lower())\n",
    "            for s in synonyms:\n",
    "                count_match_self += text.count(s.lower())\n",
    "            for acronym in acronyms:\n",
    "                # do not downcase and count acronyms\n",
    "                # otherwise you will get a lot of match of something like \"cc\", \"aa\"\n",
    "                count_match_self += raw_text.count(acronym) \n",
    "        \n",
    "            # 2) count ancestors\n",
    "            count_match_ancestor = 0\n",
    "            for a in ancestors:\n",
    "                count_match_ancestor += text.count(a.lower())\n",
    "            \n",
    "            # 3) count general descriptors\n",
    "            for v in [\"human cancer\", \"human tumor\"]:\n",
    "                count_match_ancestor += text.count(v.lower())\n",
    "        \n",
    "            # 4) count descendants\n",
    "            count_match_descendant = 0\n",
    "            for d in descendants:\n",
    "                count_match_descendant += text.count(d.lower())\n",
    "            \n",
    "            topicid = rows[\"trec_topic_number\"]\n",
    "            label = rows[\"disease_desc\"]\n",
    "            feature_table = feature_table.append({\"count_match_self\":count_match_self,\n",
    "                                                  \"count_match_ancestor\":count_match_ancestor,\n",
    "                                                  \"count_match_descendant\":count_match_descendant,\n",
    "                                                  \"topicid\":topicid,\n",
    "                                                  \"docid\":docid,\n",
    "                                                  \"label\":label},ignore_index=True)\n",
    "            \n",
    "        print(\"Topic {} has been parsed\".format(topic))\n",
    "    \n",
    "    assert df.shape[0] == feature_table.shape[0]\n",
    "    feature_table.to_csv(str(year) + \".disease.features.csv\", index=False, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate_features(2017)\n",
    "# generate_features(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_testing_classifier(training_years, testing_years):\n",
    "    \n",
    "    df_list=[]\n",
    "    for year in training_years:\n",
    "        df = pd.read_csv(str(year)+\".disease.features.csv\")\n",
    "        df_list.append(df)\n",
    "    df_training=pd.concat(df_list)\n",
    "    df_testing=pd.read_csv(str(testing_years)+\".disease.features.csv\")\n",
    "    \n",
    "    features = [\"count_match_self\", \"count_match_ancestor\", \"count_match_descendant\"]\n",
    "    training_features=df_training[features]\n",
    "    testing_features=df_testing[features]\n",
    "    training_labels=df_training.label\n",
    "    testing_labels=df_testing.label\n",
    "    \n",
    "    # over-sampling\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    training_features, training_labels = SMOTE().fit_resample(training_features, training_labels)\n",
    "    \n",
    "    # training\n",
    "    logistic_model = LogisticRegression(multi_class=\"ovr\",penalty='l1',solver='liblinear',C=0.5)\n",
    "    logistic_model.fit(training_features, training_labels)\n",
    "    \n",
    "    predicted_labels = logistic_model.predict(testing_features)\n",
    "    print(classification_report(testing_labels, predicted_labels))\n",
    "    \n",
    "    joblib.dump(logistic_model, str(testing_years)+\".disease.classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Exact       0.70      0.74      0.72      5168\n",
      " More General       0.12      0.30      0.17       686\n",
      "More Specific       0.54      0.12      0.20      1915\n",
      "  Not Disease       0.72      0.80      0.76      1455\n",
      "\n",
      "     accuracy                           0.59      9224\n",
      "    macro avg       0.52      0.49      0.46      9224\n",
      " weighted avg       0.63      0.59      0.58      9224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training_testing_classifier([2017],2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_original_distribution(training_years):\n",
    "    \n",
    "    df_list=[]\n",
    "    for year in training_years:\n",
    "        df = pd.read_csv(str(year)+\".disease.features.csv\")\n",
    "        df_list.append(df)\n",
    "    df_training=pd.concat(df_list)\n",
    "    \n",
    "    print(df_training.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact            4149\n",
      "Not Disease      2914\n",
      "More Specific    1273\n",
      "More General      938\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print_original_distribution([2017])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
