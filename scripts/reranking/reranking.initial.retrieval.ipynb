{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Users/jiamingqu/Desktop/proj/scripts/searching/evaluation.functions.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/data.modeling/training.tree.model.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/reranking/significance.testing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_prob(prob):\n",
    "    \n",
    "    '''\n",
    "    Aggregate the predicted probabilities\n",
    "    Input: An array/list of probabilities of three classes\n",
    "    Output: Aggregated probability\n",
    "    '''\n",
    "    \n",
    "    # assign weights to three classes\n",
    "    weights = [0, 2, 4]\n",
    "    \n",
    "    probability = 0\n",
    "    for i in range(0,3):\n",
    "        probability += prob[0][i] * weights[i]\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking_retrieval_results_hard(year,ndocs_rerank=500):\n",
    "    \n",
    "    \n",
    "    # read predited prob.\n",
    "    df_features_high = pd.read_csv(str(year)+'.high.features.csv',sep=',')\n",
    "    df_features_high['topicid'] = df_features_high['topicid'].astype(int)\n",
    "    df_features_high['docid'] = df_features_high['docid'].astype(str)\n",
    "\n",
    "    tree_model = joblib.load(\"../data.modeling/\"+str(year)+\".tree.model.balanced.pkl\") \n",
    "    \n",
    "                                   \n",
    "    # features \n",
    "    feature_names = ['Human_PM', 'Animal_PM', 'Not_PM', \n",
    "                     'Disease_Exact','Disease_General', 'Disease_Specific', 'Disease_Not', \n",
    "                     'Gene_Exact','Gene_Missing', 'Gene_Missing_Variant', 'Gene_Diff_Variant',\n",
    "                     'Demo_Match', 'Demo_Notdiscussed', 'Demo_Exclude']          \n",
    "                                   \n",
    "    # record results\n",
    "    precision = []\n",
    "    ap = []\n",
    "    \n",
    "    for topic in [*range(1,51)]:\n",
    "        \n",
    "        # read ground truth\n",
    "        answer = read_answers(year,topic)\n",
    "\n",
    "        # 500 results under a topic\n",
    "        df_topic = df_features_high.loc[df_features_high.topicid == topic]\n",
    "        df_topic = df_topic.head(500)\n",
    "\n",
    "        # we only rerank the first n results\n",
    "        df_topic_reanking = df_topic.head(ndocs_rerank)\n",
    "        df_topic_noreanking = df_topic.tail(500-ndocs_rerank)\n",
    "\n",
    "        # a dictionary of <docid, predicted_prob>\n",
    "        doc_prob = dict()\n",
    "        for index,rows in df_topic_reanking.iterrows():\n",
    "\n",
    "            docid = str(df_topic_reanking.loc[index,\"docid\"])\n",
    "            features = df_topic_reanking.loc[index, feature_names]\n",
    "\n",
    "            # predict and append to the dict\n",
    "            doc_prob[docid] = aggregate_prob(tree_model.predict_proba([features]))\n",
    "\n",
    "        sorted_results = sorted(doc_prob.items(), key=lambda x:x[1], reverse=True)\n",
    "        assert len(sorted_results) == ndocs_rerank\n",
    "        sorted_docs = [x[0] for x in sorted_results]\n",
    "\n",
    "        # add not reanked docs\n",
    "        sorted_docs.extend(list(df_topic_noreanking.docid))\n",
    "\n",
    "        precision_topic = calculate_precision(answer,sorted_docs,10)\n",
    "        ap_topic = calculate_average_precision(answer,sorted_docs)\n",
    "        precision.append(precision_topic)\n",
    "        ap.append(ap_topic)\n",
    "\n",
    "    return [precision,ap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking_retrieval_results_soft(year,ndocs_rerank=500):\n",
    "    \n",
    "    \n",
    "    # read predited prob.\n",
    "    df_features_high = pd.read_csv(str(year)+'.high.features.csv',sep=',')\n",
    "    df_features_high['topicid'] = df_features_high['topicid'].astype(int)\n",
    "    df_features_high['docid'] = df_features_high['docid'].astype(str)\n",
    "    \n",
    "    # features \n",
    "    feature_names = ['Human_PM', 'Animal_PM', 'Not_PM', \n",
    "                     'Disease_Exact','Disease_General', 'Disease_Specific', 'Disease_Not', \n",
    "                     'Gene_Exact','Gene_Missing', 'Gene_Missing_Variant', 'Gene_Diff_Variant',\n",
    "                     'Demo_Match', 'Demo_Notdiscussed', 'Demo_Exclude']          \n",
    "                                   \n",
    "    # record results\n",
    "    precision = []\n",
    "    ap = []\n",
    "    \n",
    "\n",
    "    for topic in [*range(1,51)]:\n",
    "        \n",
    "        # read ground truth\n",
    "        answer = read_answers(year,topic)\n",
    "\n",
    "        # 500 results under a topic\n",
    "        df_topic = df_features_high.loc[df_features_high.topicid == topic]\n",
    "        df_topic = df_topic.head(500)\n",
    "\n",
    "        # we only rerank the first n results\n",
    "        df_topic_reanking = df_topic.head(ndocs_rerank)\n",
    "        df_topic_noreanking = df_topic.tail(500-ndocs_rerank)\n",
    "\n",
    "        # a dictionary of <docid, predicted_prob>\n",
    "        doc_prob = dict()\n",
    "        for index,rows in df_topic_reanking.iterrows():\n",
    "\n",
    "            docid = str(df_topic_reanking.loc[index,\"docid\"])\n",
    "            features = df_topic_reanking.loc[index, feature_names]\n",
    "            \n",
    "            results = probalistic_tree_balanced(dict(features))\n",
    "            doc_prob[docid] = round(aggregate_prob([results]), 3)\n",
    "\n",
    "        sorted_results = sorted(doc_prob.items(), key=lambda x:x[1], reverse=True)\n",
    "        assert len(sorted_results) == ndocs_rerank\n",
    "        sorted_docs = [x[0] for x in sorted_results]\n",
    "\n",
    "        # add not reanked docs\n",
    "        sorted_docs.extend(list(df_topic_noreanking.docid))\n",
    "\n",
    "        precision_topic = calculate_precision(answer,sorted_docs,10)\n",
    "        ap_topic = calculate_average_precision(answer,sorted_docs)\n",
    "        precision.append(precision_topic)\n",
    "        ap.append(ap_topic)\n",
    "\n",
    "    return [precision,ap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranking without bm25 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574 0.24145502353628337\n"
     ]
    }
   ],
   "source": [
    "precision_tree_hard = reranking_retrieval_results_hard(2018)[0]\n",
    "ap_tree_hard = reranking_retrieval_results_hard(2018)[1]\n",
    "print(np.mean(precision_tree_hard), np.mean(ap_tree_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.562 0.23146851008452984\n"
     ]
    }
   ],
   "source": [
    "precision_tree_soft = reranking_retrieval_results_soft(2018)[0]\n",
    "ap_tree_soft = reranking_retrieval_results_soft(2018)[1]\n",
    "print(np.mean(precision_tree_soft), np.mean(ap_tree_soft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranking with bm25 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking_retrieval_results_hard_withbm25(year,ndocs_rerank=500):\n",
    "    \n",
    "    # read predited prob.\n",
    "    df_features_high = pd.read_csv(str(year)+'.high.features.csv',sep=',')\n",
    "    df_features_high['topicid'] = df_features_high['topicid'].astype(int)\n",
    "    df_features_high['docid'] = df_features_high['docid'].astype(str)\n",
    "    \n",
    "    initial_retrieval = pd.read_csv(\"../searching/\"+str(year)+\".searching/\"+str(year)+\".basic.query.result.txt\",sep=\"\\t\")\n",
    "    df_features_high['score'] = initial_retrieval['SCORE']\n",
    "    \n",
    "    # read tree model\n",
    "    tree_model = joblib.load(\"../data.modeling/\"+str(year)+\".tree.model.balanced.pkl\")\n",
    "    \n",
    "                                   \n",
    "    # features \n",
    "    feature_names = ['Human_PM', 'Animal_PM', 'Not_PM', \n",
    "                     'Disease_Exact','Disease_General', 'Disease_Specific', 'Disease_Not', \n",
    "                     'Gene_Exact','Gene_Missing', 'Gene_Missing_Variant', 'Gene_Diff_Variant',\n",
    "                     'Demo_Match', 'Demo_Notdiscussed', 'Demo_Exclude']          \n",
    "                                   \n",
    "    # record results\n",
    "    precision = []\n",
    "    ap = []\n",
    "    recall = []\n",
    "    rprec = []\n",
    "    \n",
    "    for topic in set(df_features_high.topicid):\n",
    "        \n",
    "        # read ground truth\n",
    "        answer = read_answers(year,topic)\n",
    "\n",
    "        # 500 results under a topic\n",
    "        df_topic = df_features_high.loc[df_features_high.topicid == topic]\n",
    "        df_topic = df_topic.head(500)\n",
    "   \n",
    "        # we only rerank the first n results\n",
    "        df_topic_reanking = df_topic.head(ndocs_rerank)\n",
    "        df_topic_noreanking = df_topic.tail(500-ndocs_rerank)\n",
    "        \n",
    "        df_score_reranking = pd.DataFrame(columns = ['docid','tree_prob', 'bm25'])\n",
    "\n",
    "        # for reranking docs\n",
    "        for index,rows in df_topic_reanking.iterrows():\n",
    "            \n",
    "            docid = str(df_topic_reanking.loc[index,\"docid\"])\n",
    "            \n",
    "            # load intial retrieval scores\n",
    "            bm25_score = df_topic_reanking.loc[index,\"score\"]\n",
    "            \n",
    "            # using features to predict\n",
    "            features = df_topic_reanking.loc[index, feature_names]\n",
    "            tree_score = aggregate_prob(tree_model.predict_proba([features]))\n",
    "\n",
    "            # add to dataframe\n",
    "            df_score_reranking = df_score_reranking.append({'docid': docid,\n",
    "                                                            'tree_prob': tree_score, \n",
    "                                                            'bm25': bm25_score},ignore_index=True)\n",
    "            \n",
    "        assert df_score_reranking.shape[0] == ndocs_rerank \n",
    "\n",
    "        # for unrerank docs\n",
    "        for index,rows in df_topic_noreanking.iterrows():\n",
    "            \n",
    "            docid = str(df_topic_noreanking.loc[index,\"docid\"])\n",
    "            # load intial retrieval scores\n",
    "            bm25_score = df_topic_noreanking.loc[index,\"score\"]\n",
    "            # add to dataframe\n",
    "            df_score_reranking = df_score_reranking.append({'docid': docid,\n",
    "                                                            'tree_prob': 0, \n",
    "                                                            'bm25': bm25_score},ignore_index=True)\n",
    "            \n",
    "        assert df_score_reranking.shape[0] == 500\n",
    "        \n",
    "        # scaling and adding two score\n",
    "        scaler = MinMaxScaler()\n",
    "        #df_score_reranking[['tree_prob','bm25']]=scaler.fit_transform(df_score_reranking[['tree_prob','bm25']])\n",
    "        df_score_reranking['total_score'] = df_score_reranking['tree_prob'] + df_score_reranking['bm25']\n",
    "        df_score_reranking.sort_values(by=['total_score'],inplace=True, ascending=False)\n",
    "        sorted_docs = list(df_score_reranking.docid)\n",
    "\n",
    "        precision.append(calculate_precision(answer,sorted_docs,10))\n",
    "        ap.append(calculate_average_precision(answer,sorted_docs))\n",
    "        recall.append(calculate_recall(answer,sorted_docs,100))\n",
    "        rprec.append(calculate_r_precision(answer,sorted_docs))\n",
    "\n",
    "    return [precision, ap, recall, rprec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking_retrieval_results_soft_withbm25(year,ndocs_rerank=500):\n",
    "    \n",
    "    # read predited prob.\n",
    "    df_features_high = pd.read_csv(str(year)+'.high.features.csv',sep=',')\n",
    "    df_features_high['topicid'] = df_features_high['topicid'].astype(int)\n",
    "    df_features_high['docid'] = df_features_high['docid'].astype(str)\n",
    "    \n",
    "    initial_retrieval = pd.read_csv(\"../searching/\"+str(year)+\".searching/\"+str(year)+\".basic.query.result.txt\",sep=\"\\t\")\n",
    "    df_features_high['score'] = initial_retrieval['SCORE']\n",
    "                                   \n",
    "    # features \n",
    "    feature_names = ['Human_PM', 'Animal_PM', 'Not_PM', \n",
    "                     'Disease_Exact','Disease_General', 'Disease_Specific', 'Disease_Not', \n",
    "                     'Gene_Exact','Gene_Missing', 'Gene_Missing_Variant', 'Gene_Diff_Variant',\n",
    "                     'Demo_Match', 'Demo_Notdiscussed', 'Demo_Exclude']          \n",
    "                                   \n",
    "    # record results\n",
    "    # record results\n",
    "    precision = []\n",
    "    ap = []\n",
    "    recall = []\n",
    "    rprec = []\n",
    "    \n",
    "    for topic in set(df_features_high.topicid):\n",
    "        \n",
    "        # read ground truth\n",
    "        answer = read_answers(year,topic)\n",
    "\n",
    "        # 500 results under a topic\n",
    "        df_topic = df_features_high.loc[df_features_high.topicid == topic]\n",
    "        df_topic = df_topic.head(500)\n",
    "        \n",
    "        # we only rerank the first n results\n",
    "        df_topic_reanking = df_topic.head(ndocs_rerank)\n",
    "        df_topic_noreanking = df_topic.tail(500-ndocs_rerank)\n",
    "        \n",
    "        df_score_reranking = pd.DataFrame(columns = ['docid','tree_prob', 'bm25'])\n",
    "\n",
    "        # for reranking docs\n",
    "        for index,rows in df_topic_reanking.iterrows():\n",
    "            \n",
    "            docid = str(df_topic_reanking.loc[index,\"docid\"])\n",
    "            \n",
    "            # load intial retrieval scores\n",
    "            bm25_score = df_topic_reanking.loc[index,\"score\"]\n",
    "            \n",
    "            # using features to predict\n",
    "            features = df_topic_reanking.loc[index, feature_names]\n",
    "            results = probalistic_tree_balanced(dict(features))\n",
    "            tree_score = aggregate_prob_soft([results])\n",
    "\n",
    "            # add to dataframe\n",
    "            df_score_reranking = df_score_reranking.append({'docid': docid,\n",
    "                                                            'tree_prob': tree_score, \n",
    "                                                            'bm25': bm25_score},ignore_index=True)\n",
    "            \n",
    "        assert df_score_reranking.shape[0] == ndocs_rerank \n",
    "\n",
    "        # for unrerank docs\n",
    "        for index,rows in df_topic_noreanking.iterrows():\n",
    "            \n",
    "            docid = str(df_topic_noreanking.loc[index,\"docid\"])\n",
    "            # load intial retrieval scores\n",
    "            bm25_score = df_topic_noreanking.loc[index,\"score\"]\n",
    "            # add to dataframe\n",
    "            df_score_reranking = df_score_reranking.append({'docid': docid,\n",
    "                                                            'tree_prob': 0, \n",
    "                                                            'bm25': bm25_score},ignore_index=True)\n",
    "            \n",
    "        assert df_score_reranking.shape[0] == 500\n",
    "        \n",
    "        # scaling and adding two score\n",
    "        scaler = MinMaxScaler()\n",
    "        df_score_reranking[['tree_prob','bm25']]=scaler.fit_transform(df_score_reranking[['tree_prob','bm25']])\n",
    "        df_score_reranking['total_score'] = df_score_reranking['tree_prob'] + df_score_reranking['bm25']\n",
    "        df_score_reranking.sort_values(by=['total_score'],inplace=True, ascending=False)\n",
    "        sorted_docs = list(df_score_reranking.docid)\n",
    "\n",
    "        precision.append(calculate_precision(answer,sorted_docs,10))\n",
    "        ap.append(calculate_average_precision(answer,sorted_docs))\n",
    "        recall.append(calculate_recall(answer,sorted_docs,100))\n",
    "        rprec.append(calculate_r_precision(answer,sorted_docs))\n",
    "\n",
    "    return [precision, ap, recall, rprec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bm25 by min max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_tree_hard_bm25, ap_tree_hard_bm25, recall_tree_hard_bm25, rprec_tree_hard_bm25 = \\\n",
    "    reranking_retrieval_results_hard_withbm25(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_tree_soft_bm25, ap_tree_soft_bm25, recall_tree_soft_bm25, rprec_tree_soft_bm25 = \\\n",
    "    reranking_retrieval_results_soft_withbm25(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.546, 0.2377849187143844, 0.3344221502141547, 0.3232313153621363)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision_tree_hard_bm25), np.mean(ap_tree_hard_bm25), np.mean(recall_tree_hard_bm25), np.mean(rprec_tree_hard_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.622, 0.260482763795428, 0.3600719402556245, 0.34631718256635335)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision_tree_soft_bm25), np.mean(ap_tree_soft_bm25), np.mean(recall_tree_soft_bm25), np.mean(rprec_tree_soft_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard vs soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00058"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_permutation_test(precision_tree_hard_bm25, precision_tree_soft_bm25, isTwoSides=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_permutation_test(ap_tree_hard_bm25, ap_tree_soft_bm25, isTwoSides=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_permutation_test(rprec_tree_hard_bm25, rprec_tree_soft_bm25, isTwoSides=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltr-low vs soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_permutation_test(precision_ltr_low, ap_tree_soft_bm25, isTwoSides=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2e-05"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_permutation_test(ap_ltr_low, ap_tree_soft_bm25, isTwoSides=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00559"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_permutation_test(rprec_ltr_low, rprec_tree_soft_bm25, isTwoSides=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
