{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Users/jiamingqu/Desktop/proj/scripts/searching/evaluation.functions.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/read.dataframe.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/demo/demo.classifier.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/gene/gene.classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demo_features(text, topic, demo_classifier, query_topics_demo):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features = dict()\n",
    "    features_low = dict()\n",
    "    features_high = dict()\n",
    "    \n",
    "    # load query topics and classifier\n",
    "    demo = query_topics_demo[topic]\n",
    "    age = int(demo.split(\"-\")[0])\n",
    "    gender = demo.split(\" \")[1]\n",
    "    \n",
    "    # check numeric age\n",
    "    age_numeric = check_age_diff_numeric(age, text)\n",
    "    if age_numeric == \"MissingNumericAge\":\n",
    "        (age_numeric_missing, age_numeric_diff) = (1, 0)\n",
    "    else:\n",
    "        (age_numeric_missing, age_numeric_diff) = (0, age_numeric)\n",
    "\n",
    "    # check text age\n",
    "    age_text = count_age_group_keywords_text(age, text)\n",
    "    if age_text == \"MissingTextAge\":\n",
    "        (age_text_missing, age_text_match) = (1, 0)\n",
    "    else:\n",
    "        (age_text_missing, age_text_match) = (0, age_text)\n",
    "\n",
    "    # check geneder\n",
    "    gender_check = check_gender_diff(gender, text)\n",
    "    if gender_check == \"MissingGender\":\n",
    "        (gender_missing, gender_diff) = (1, 0)\n",
    "    else:\n",
    "        (gender_missing, gender_diff) = (0, gender_check)\n",
    "    \n",
    "    # add to low level features dictionary\n",
    "    features_low['age_missing_numeric'] = age_numeric_missing\n",
    "    features_low['age_diff_numeric'] = age_numeric_diff\n",
    "    features_low['age_missing_text'] = age_text_missing\n",
    "    features_low['age_match_text'] = age_text_match\n",
    "    features_low['gender_missing'] = gender_missing\n",
    "    features_low['gender_diff'] = gender_diff\n",
    "    features['low'] = features_low\n",
    "    \n",
    "    # add high level features, i.e., probabilities\n",
    "    feature_vector = [age_numeric_diff,age_numeric_missing,age_text_missing,age_text_match,gender_diff,gender_missing]\n",
    "    prob = demo_classifier.predict_proba([feature_vector])[0]\n",
    "    \n",
    "    \n",
    "    demo_exclude_prob = prob[0]\n",
    "    demo_match_prob = prob[1]\n",
    "    demo_not_prob = prob[2]\n",
    "    \n",
    "        \n",
    "    a = demo_exclude_prob/(7126/815)\n",
    "    b = demo_match_prob/(7126/607)\n",
    "    c = demo_not_prob/(7126/7126)\n",
    "\n",
    "    adjusted_demo_exclude_prob = a/(a+b+c)\n",
    "    adjusted_demo_match_prob = b/(a+b+c)\n",
    "    adjusted_demo_not_prob = c/(a+b+c)\n",
    "\n",
    "    \n",
    "    # in accord with labels\n",
    "    features_high['Demo_Exclude'] = adjusted_demo_exclude_prob\n",
    "    features_high['Demo_Match'] = adjusted_demo_match_prob\n",
    "    features_high['Demo_Notdiscussed'] = adjusted_demo_not_prob\n",
    "    features['high'] = features_high\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disease_features(raw_text, topic, disease_classifier, query_topics_disease, disease_expansion_terms, acronyms_dict):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features = dict()\n",
    "    features_low = dict()\n",
    "    features_high = dict()\n",
    "    \n",
    "    \n",
    "    disease = query_topics_disease[topic]\n",
    "    \n",
    "    expansion_terms = disease_expansion_terms[str(topic)]\n",
    "    synonyms = expansion_terms[\"synonyms\"]\n",
    "    ancestors = expansion_terms[\"ancestors\"]\n",
    "    descendants = expansion_terms[\"descendants\"]\n",
    "    if disease in acronyms_dict.keys():\n",
    "        acronyms = acronyms_dict[disease].split(\" \")\n",
    "    else:\n",
    "        acronyms = []\n",
    "    \n",
    "    # start generating features\n",
    "    text = raw_text.lower()\n",
    "    count_match_self,count_match_ancestor,count_match_descendant = 0,0,0\n",
    "    \n",
    "    # 1) count disease itself\n",
    "    count_match_self = text.count(disease.lower())\n",
    "    for s in synonyms:\n",
    "        count_match_self += text.count(s.lower())\n",
    "    for acronym in acronyms:\n",
    "        # do not downcase and count acronyms\n",
    "        # otherwise you will get a lot of match of something like \"cc\", \"aa\"\n",
    "        count_match_self += raw_text.count(acronym) \n",
    "\n",
    "    # 2) count ancestors\n",
    "    for a in ancestors:\n",
    "        count_match_ancestor += text.count(a.lower())\n",
    "\n",
    "    # 3) count general descriptors\n",
    "    for v in [\"human cancer\", \"human tumor\"]:\n",
    "        count_match_ancestor += text.count(v.lower())\n",
    "\n",
    "    # 4) count descendants\n",
    "    for d in descendants:\n",
    "        count_match_descendant += text.count(d.lower())\n",
    "        \n",
    "    # add to low level features dictionary\n",
    "    features_low['count_match_self'] = count_match_self\n",
    "    features_low['count_match_ancestor'] = count_match_ancestor\n",
    "    features_low['count_match_descendant'] = count_match_descendant\n",
    "    features['low'] = features_low\n",
    "    \n",
    "    # add high level features, i.e., probabilities\n",
    "    feature_vector = [count_match_self,count_match_ancestor,count_match_descendant]\n",
    "    prob = disease_classifier.predict_proba([feature_vector])[0]\n",
    "    \n",
    "    \n",
    "    exact_prob = prob[0]\n",
    "    general_prob = prob[1]\n",
    "    specific_prob = prob[2]\n",
    "    not_prob = prob[3]\n",
    "    \n",
    "    a = exact_prob/(4149/4149)\n",
    "    b = general_prob/(4149/938)\n",
    "    c = specific_prob/(4149/1273)\n",
    "    d = not_prob/(4149/2914)\n",
    "\n",
    "    adjusted_exact_prob = a/(a+b+c+d)\n",
    "    adjusted_general_prob = b/(a+b+c+d)\n",
    "    adjusted_specific_prob = c/(a+b+c+d)\n",
    "    adjusted_not_prob =  d/(a+b+c+d)\n",
    "\n",
    "    \n",
    "    # in accord with labels\n",
    "    features_high['Disease_Exact'] = adjusted_exact_prob\n",
    "    features_high['Disease_General'] = adjusted_general_prob\n",
    "    features_high['Disease_Specific'] = adjusted_specific_prob\n",
    "    features_high['Disease_Not'] = adjusted_not_prob\n",
    "    features['high'] = features_high\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation and new line characters\n",
    "    text.replace(\"\\t\",\" \")\n",
    "    text.replace(\"\\n\",\" \")\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    # remove digits\n",
    "    text = re.sub(r\"\\b\\d+\\b\",\" \", text)\n",
    "    # remove multiple white spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    # remove stopwords\n",
    "    text = [x for x in text.split() if x not in stop]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pm_features(text, pm_classifier):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features = dict()\n",
    "    features_low = dict()\n",
    "    features_high = dict()\n",
    "    \n",
    "    # first clean text\n",
    "    text = clean_text(text)\n",
    "\n",
    "    \n",
    "    keyword_animal = ['mice', 'mouse', 'model', 'mammary', \n",
    "                      'rat','xenografts','dog','canie'\n",
    "                      'vivo','cycle','mutated','preclinical',\n",
    "                      'prostate','pten','liver','met','animal',\n",
    "                      'mgkg','human', 'xenograft']\n",
    "\n",
    "    keyword_human = ['gastrectomy','imatinib','gastric','stomach',\n",
    "                 'fgfr1','prognostic','mutation','gastrointestinal',\n",
    "                 'mutations','families','shorter','inhibitor',\n",
    "                 'kit','located','lethal','kras','dose',\n",
    "                 'tract','pfs','mutated']\n",
    "\n",
    "    keyword_not = ['transplantation','symptoms','female','male',\n",
    "                   'driver','pressure','pancreaticoduodenectomy',\n",
    "                   'surface','triple','women',\n",
    "                   'a549','mortality','adjuvant',\n",
    "                   'bypass','basis','myxoid']\n",
    "        \n",
    "    match_human = 0\n",
    "    match_animal = 0\n",
    "    match_not = 0\n",
    "    \n",
    "    for k in keyword_human:\n",
    "        match_human += text.count(k)\n",
    "        \n",
    "    for k in keyword_animal:\n",
    "        match_animal += text.count(k)\n",
    "            \n",
    "    for k in keyword_not:\n",
    "        match_not += text.count(k)\n",
    "\n",
    "    features_low['match_human'] = match_human\n",
    "    features_low['match_animal'] = match_animal\n",
    "    features_low['match_not'] = match_not\n",
    "    features['low'] = features_low\n",
    "\n",
    "    \n",
    "    feature_vector = [match_human,match_animal,match_not]\n",
    "    prob = pm_classifier.predict_proba([feature_vector])[0]\n",
    "    \n",
    "    # adjust predicted prob bc we used SMOTE oversampling during training\n",
    "    animal_prob = prob[0]\n",
    "    human_prob = prob[1]\n",
    "    not_prob = prob[2]\n",
    "\n",
    "        \n",
    "    a = animal_prob/(13368/536)\n",
    "    b = human_prob/(13368/8738)\n",
    "    c = not_prob/(13368/13368)\n",
    "\n",
    "\n",
    "    adjusted_animal_prob = a/(a+b+c)\n",
    "    adjusted_human_prob = b/(a+b+c)\n",
    "    adjusted_not_prob = c/(a+b+c)\n",
    "        \n",
    "    \n",
    "    features_high['Animal_PM'] = adjusted_animal_prob\n",
    "    features_high['Human_PM'] = adjusted_human_prob\n",
    "    features_high['Not_PM'] = adjusted_not_prob\n",
    "    features['high'] = features_high\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gene_features(text, topic, gene_classifier, query_topics_gene):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features = dict()\n",
    "    features_low = dict()\n",
    "    features_high = dict()\n",
    "    \n",
    "    query_genes = query_topics_gene[topic]\n",
    "    \n",
    "    # there might be several genes in the query\n",
    "    # we take the average for both low and mid level features\n",
    "    match_gene_list = []\n",
    "    has_variation_list = []\n",
    "    match_variation_list = []\n",
    "    match_total_variation_list = []\n",
    "    has_other_info_list = []\n",
    "    match_other_info_list = []\n",
    "    \n",
    "    differ_variant_list = []\n",
    "    exact_list = []\n",
    "    missing_gene_list = []\n",
    "    missing_variant_list = []\n",
    "    \n",
    "    for query_gene in query_genes.split(\",\"):\n",
    "        \n",
    "        query_gene = query_gene.strip()\n",
    "        genes,variant,other_info = parsing_query_gene(query_gene)\n",
    "        \n",
    "        # 1) match gene\n",
    "        match_gene = 0\n",
    "        for gene in genes:\n",
    "            if \" \" not in gene: \n",
    "                match_gene += text.count(gene)\n",
    "                aliases = get_gene_alias_and_variations(gene)[\"aliases\"]\n",
    "                for alias in aliases:\n",
    "                    match_gene += text.count(alias)\n",
    "            else: # special case: the gene is a phrase\n",
    "                for g in gene.split(\" \"):\n",
    "                    g = g.strip()\n",
    "                    match_gene += text.lower().count(g.lower())\n",
    "\n",
    "\n",
    "        # 2) match variation\n",
    "        if variant != 0:\n",
    "            has_variation,match_variation,match_total_variation = 1,0,0\n",
    "            # match variation\n",
    "            match_variation += text.count(variant)\n",
    "            # match all the variations\n",
    "            for gene in genes:\n",
    "                variants = get_gene_alias_and_variations(gene)[\"variants\"]\n",
    "                for v in variants:\n",
    "                    match_total_variation += text.count(v)\n",
    "        else:\n",
    "            has_variation,match_variation,match_total_variation = 0,0,0\n",
    "            # match all the variations\n",
    "            for gene in genes:\n",
    "                variants = get_gene_alias_and_variations(gene)[\"variants\"]\n",
    "                for v in variants:\n",
    "                    match_total_variation += text.count(v)\n",
    "\n",
    "\n",
    "        # 3) match other info\n",
    "        if other_info != 0:\n",
    "            has_other_info, match_other_info = 1,0\n",
    "            for o in other_info.split(\" \"):\n",
    "                o = o.strip()\n",
    "                match_other_info += text.lower().count(o.lower())\n",
    "        else:\n",
    "            has_other_info, match_other_info = 0,0\n",
    "            \n",
    "        match_gene_list.append(match_gene)\n",
    "        has_variation_list.append(has_variation)\n",
    "        match_variation_list.append(match_variation)\n",
    "        match_total_variation_list.append(match_total_variation)\n",
    "        has_other_info_list.append(has_other_info)\n",
    "        match_other_info_list.append(match_other_info)\n",
    "        \n",
    "        # make predictions\n",
    "        feature_vector = [match_gene,has_variation,match_variation,match_total_variation,has_other_info,match_other_info]\n",
    "        prob = gene_classifier.predict_proba([feature_vector])[0]\n",
    "        \n",
    "        \n",
    "        differ_variant_prob = prob[0]\n",
    "        exact_prob = prob[1]\n",
    "        missing_gene_prob = prob[2]\n",
    "        missing_variant_prob = prob[3]\n",
    "        \n",
    "\n",
    "        a = differ_variant_prob/(4578/602)\n",
    "        b = exact_prob/(4578/4578)\n",
    "        c = missing_gene_prob/(4578/3696)\n",
    "        d = missing_variant_prob/(4578/1551)\n",
    "\n",
    "        adjusted_differ_variant_prob = a/(a+b+c+d)\n",
    "        adjusted_exact_prob = b/(a+b+c+d)\n",
    "        adjusted_missing_gene_prob = c/(a+b+c+d)\n",
    "        adjusted_missing_variant_prob = d/(a+b+c+d)\n",
    "            \n",
    "\n",
    "        differ_variant_list.append(adjusted_differ_variant_prob)\n",
    "        exact_list.append(adjusted_exact_prob)\n",
    "        missing_gene_list.append(adjusted_missing_gene_prob)\n",
    "        missing_variant_list.append(adjusted_missing_variant_prob)\n",
    "        \n",
    "        \n",
    "    # add the average to dictionary\n",
    "    features_low['match_gene'] = np.mean(match_gene_list)\n",
    "    features_low['has_variation'] = np.mean(has_variation_list)\n",
    "    features_low['match_variation'] = np.mean(match_variation_list)\n",
    "    features_low['match_total_variation'] = np.mean(match_total_variation_list)\n",
    "    features_low['has_other_info'] = np.mean(has_other_info_list)\n",
    "    features_low['match_other_info'] = np.mean(match_other_info_list)\n",
    "    features['low'] = features_low\n",
    "    \n",
    "    features_high['Gene_Diff_Variant'] = np.mean(differ_variant_list)\n",
    "    features_high['Gene_Exact'] = np.mean(exact_list)\n",
    "    features_high['Gene_Missing'] = np.mean(missing_gene_list)\n",
    "    features_high['Gene_Missing_Variant'] = np.mean(missing_variant_list)\n",
    "    features['high'] = features_high\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reranking_features(year):\n",
    "    \n",
    "    '''\n",
    "    Generate reranking features for both low-level and mid-level\n",
    "    low-level: raw features\n",
    "    mid-level: predicted prob\n",
    "    '''\n",
    "    \n",
    "    initial_retrieval = pd.read_csv(\"../../\"+str(year)+\".basic.query.result.txt\",sep=\"\\t\")\n",
    "    initial_retrieval.ID = initial_retrieval.ID.astype(str)\n",
    "    \n",
    "    \n",
    "    # load classifiers and useful resources\n",
    "    # disease\n",
    "    query_topics_disease = read_query_topics(year,\"disease\")\n",
    "    disease_classifier = joblib.load(\"../classifier/disease/\" + str(year) + \".disease.classifier.pkl\")\n",
    "    disease_expansion_terms = dict()\n",
    "    with open (\"../classifier/disease/\" + str(year) + \".disease.expansion.json\", 'r') as f:\n",
    "        for data in f:\n",
    "            disease_expansion_terms = json.loads(data)\n",
    "    f.close()\n",
    "    # read acronyms: a dict of <disease, acronyms>\n",
    "    acronyms_dict = dict()\n",
    "    with open(\"../classifier/disease/acronyms.json\",'r') as f:\n",
    "         for line in f.readlines():\n",
    "            acronyms_dict = json.loads(line)\n",
    "    f.close()\n",
    "    # demo\n",
    "    query_topics_demo = read_query_topics(year,\"demo\")\n",
    "    demo_classifier = joblib.load(\"../classifier/demo/\" + str(year) + \".demo.classifier.pkl\")\n",
    "    # gene\n",
    "    query_topics_gene = read_query_topics(year,\"gene\")\n",
    "    gene_classifier = joblib.load(\"../classifier/gene/\" + str(year) + \".gene.classifier.pkl\")\n",
    "    # pm\n",
    "    pm_classifier = joblib.load(\"../classifier/pm/\"+str(year)+\".pm.classifier.pkl\")\n",
    "    \n",
    "\n",
    "    # two dataframes of two levels of features\n",
    "    df_low = pd.DataFrame(columns=['count_match_self', 'count_match_ancestor', 'count_match_descendant', \n",
    "                                    'age_missing_numeric', 'age_diff_numeric', 'age_missing_text',\n",
    "                                    'age_match_text', 'gender_missing', 'gender_diff',\n",
    "                                    'match_gene', 'has_variation', 'match_variation', \n",
    "                                    'match_total_variation', 'has_other_info', 'match_other_info',\n",
    "                                    'match_human','match_animal','match_not',\n",
    "                                    'topicid','docid','year'])\n",
    "    \n",
    "    df_high = pd.DataFrame(columns=['Human_PM', 'Animal_PM', 'Not_PM', 'Disease_Exact',\n",
    "                                   'Disease_General', 'Disease_Specific', 'Disease_Not', \n",
    "                                   'Gene_Exact','Gene_Missing', 'Gene_Missing_Variant', 'Gene_Diff_Variant',\n",
    "                                   'Demo_Match', 'Demo_Notdiscussed', 'Demo_Exclude',\n",
    "                                   'topicid','docid','year'])\n",
    "            \n",
    "    for topic in set(initial_retrieval.TOPIC_NO):\n",
    "        \n",
    "        print(\"Parsing topic No. {}\".format(topic))\n",
    "        result_by_topic = initial_retrieval.loc[initial_retrieval.TOPIC_NO==topic]\n",
    "        \n",
    "        for index,rows in result_by_topic.iterrows():\n",
    "            \n",
    "            docid = result_by_topic.loc[index,\"ID\"]\n",
    "            title = result_by_topic.loc[index,\"TITLE\"]\n",
    "            content = result_by_topic.loc[index,\"CONTENT\"]\n",
    "            text = title + \" \" + content\n",
    "            \n",
    "            # generate pm features\n",
    "            pm_features = generate_pm_features(text,pm_classifier)\n",
    "            pm_low = pm_features[\"low\"]\n",
    "            pm_high = pm_features[\"high\"]\n",
    "            \n",
    "            # generate demo features\n",
    "            demo_features = generate_demo_features(text,topic,demo_classifier,query_topics_demo)\n",
    "            demo_low = demo_features[\"low\"]\n",
    "            demo_high = demo_features[\"high\"]\n",
    "            \n",
    "            # generate disease features\n",
    "            disease_features = generate_disease_features(text,topic,disease_classifier,\\\n",
    "                                                    query_topics_disease,disease_expansion_terms,acronyms_dict)\n",
    "            disease_low = disease_features[\"low\"]\n",
    "            disease_high = disease_features[\"high\"]\n",
    "            \n",
    "            # generate gene features\n",
    "            gene_fearures = generate_gene_features(text,topic,gene_classifier,query_topics_gene)\n",
    "            gene_low = gene_fearures[\"low\"]\n",
    "            gene_high = gene_fearures[\"high\"]\n",
    "            \n",
    "            # add features and append to the dataframe\n",
    "            record_low = dict(**pm_low, **demo_low, **disease_low, **gene_low)\n",
    "            record_low['topicid'] = topic\n",
    "            record_low['docid'] = docid\n",
    "            record_low['year'] = year\n",
    "            \n",
    "            record_high = dict(**pm_high, **demo_high, **disease_high, **gene_high)\n",
    "            record_high['topicid'] = topic\n",
    "            record_high['docid'] = docid\n",
    "            record_high['year'] = year\n",
    "            \n",
    "            \n",
    "            df_low = df_low.append(record_low, ignore_index=True)\n",
    "            df_high = df_high.append(record_high, ignore_index=True)\n",
    "    \n",
    "\n",
    "    # save to csv\n",
    "    df_low.to_csv(str(year)+\".low.features.csv\", sep=\",\", index=False)\n",
    "    df_high.to_csv(str(year)+\".high.features.csv\", sep=\",\", index=False)\n",
    "    \n",
    "    print(df_low.shape)\n",
    "    print(df_high.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing topic No. 1\n",
      "Parsing topic No. 2\n",
      "Parsing topic No. 3\n",
      "Parsing topic No. 4\n",
      "Parsing topic No. 5\n",
      "Parsing topic No. 6\n",
      "Parsing topic No. 7\n",
      "Parsing topic No. 8\n",
      "Parsing topic No. 9\n",
      "Parsing topic No. 10\n",
      "Parsing topic No. 11\n",
      "Parsing topic No. 12\n",
      "Parsing topic No. 13\n",
      "Parsing topic No. 14\n",
      "Parsing topic No. 15\n",
      "Parsing topic No. 16\n",
      "Parsing topic No. 17\n",
      "Parsing topic No. 18\n",
      "Parsing topic No. 19\n",
      "Parsing topic No. 20\n",
      "Parsing topic No. 21\n",
      "Parsing topic No. 22\n",
      "Parsing topic No. 23\n",
      "Parsing topic No. 24\n",
      "Parsing topic No. 25\n",
      "Parsing topic No. 26\n",
      "Parsing topic No. 27\n",
      "Parsing topic No. 28\n",
      "Parsing topic No. 29\n",
      "Parsing topic No. 30\n",
      "Parsing topic No. 31\n",
      "Parsing topic No. 32\n",
      "Parsing topic No. 33\n",
      "Parsing topic No. 34\n",
      "Parsing topic No. 35\n",
      "Parsing topic No. 36\n",
      "Parsing topic No. 37\n",
      "Parsing topic No. 38\n",
      "Parsing topic No. 39\n",
      "Parsing topic No. 40\n",
      "Parsing topic No. 41\n",
      "Parsing topic No. 42\n",
      "Parsing topic No. 43\n",
      "Parsing topic No. 44\n",
      "Parsing topic No. 45\n",
      "Parsing topic No. 46\n",
      "Parsing topic No. 47\n",
      "Parsing topic No. 48\n",
      "Parsing topic No. 49\n",
      "Parsing topic No. 50\n",
      "(25000, 21)\n",
      "(25000, 17)\n"
     ]
    }
   ],
   "source": [
    "# generate_reranking_features(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
