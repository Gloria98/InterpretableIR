{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/read.dataframe.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/demo/demo.classifier.ipynb\n",
    "%run /Users/jiamingqu/Desktop/proj/scripts/classifier/gene/gene.classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demo_features(text, topic, query_topics_demo):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features_low = dict()\n",
    "\n",
    "    \n",
    "    # load query topics and classifier\n",
    "    demo = query_topics_demo[topic]\n",
    "    age = int(demo.split(\"-\")[0])\n",
    "    gender = demo.split(\" \")[1]\n",
    "    \n",
    "    # check numeric age\n",
    "    age_numeric = check_age_diff_numeric(age, text)\n",
    "    if age_numeric == \"MissingNumericAge\":\n",
    "        (age_numeric_missing, age_numeric_diff) = (1, 0)\n",
    "    else:\n",
    "        (age_numeric_missing, age_numeric_diff) = (0, age_numeric)\n",
    "\n",
    "    # check text age\n",
    "    age_text = count_age_group_keywords_text(age, text)\n",
    "    if age_text == \"MissingTextAge\":\n",
    "        (age_text_missing, age_text_match) = (1, 0)\n",
    "    else:\n",
    "        (age_text_missing, age_text_match) = (0, age_text)\n",
    "\n",
    "    # check geneder\n",
    "    gender_check = check_gender_diff(gender, text)\n",
    "    if gender_check == \"MissingGender\":\n",
    "        (gender_missing, gender_diff) = (1, 0)\n",
    "    else:\n",
    "        (gender_missing, gender_diff) = (0, gender_check)\n",
    "    \n",
    "    # add to low level features dictionary\n",
    "    features_low['age_missing_numeric'] = age_numeric_missing\n",
    "    features_low['age_diff_numeric'] = age_numeric_diff\n",
    "    features_low['age_missing_text'] = age_text_missing\n",
    "    features_low['age_match_text'] = age_text_match\n",
    "    features_low['gender_missing'] = gender_missing\n",
    "    features_low['gender_diff'] = gender_diff\n",
    "   \n",
    "    return features_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disease_features(raw_text, topic, query_topics_disease, disease_expansion_terms, acronyms_dict):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features_low = dict()\n",
    "    \n",
    "    disease = query_topics_disease[topic]\n",
    "    \n",
    "    expansion_terms = disease_expansion_terms[str(topic)]\n",
    "    synonyms = expansion_terms[\"synonyms\"]\n",
    "    ancestors = expansion_terms[\"ancestors\"]\n",
    "    descendants = expansion_terms[\"descendants\"]\n",
    "    if disease in acronyms_dict.keys():\n",
    "        acronyms = acronyms_dict[disease].split(\" \")\n",
    "    else:\n",
    "        acronyms = []\n",
    "    \n",
    "    # start generating features\n",
    "    text = raw_text.lower()\n",
    "    count_match_self,count_match_ancestor,count_match_descendant = 0,0,0\n",
    "    \n",
    "    # 1) count disease itself\n",
    "    count_match_self = text.count(disease.lower())\n",
    "    for s in synonyms:\n",
    "        count_match_self += text.count(s.lower())\n",
    "    for acronym in acronyms:\n",
    "        # do not downcase and count acronyms\n",
    "        # otherwise you will get a lot of match of something like \"cc\", \"aa\"\n",
    "        count_match_self += raw_text.count(acronym) \n",
    "\n",
    "    # 2) count ancestors\n",
    "    for a in ancestors:\n",
    "        count_match_ancestor += text.count(a.lower())\n",
    "\n",
    "    # 3) count general descriptors\n",
    "    for v in [\"human cancer\", \"human tumor\"]:\n",
    "        count_match_ancestor += text.count(v.lower())\n",
    "\n",
    "    # 4) count descendants\n",
    "    for d in descendants:\n",
    "        count_match_descendant += text.count(d.lower())\n",
    "        \n",
    "    # add to low level features dictionary\n",
    "    features_low['count_match_self'] = count_match_self\n",
    "    features_low['count_match_ancestor'] = count_match_ancestor\n",
    "    features_low['count_match_descendant'] = count_match_descendant\n",
    "    \n",
    "    \n",
    "    return features_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation and new line characters\n",
    "    text.replace(\"\\t\",\" \")\n",
    "    text.replace(\"\\n\",\" \")\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    # remove digits\n",
    "    text = re.sub(r\"\\b\\d+\\b\",\" \", text)\n",
    "    # remove multiple white spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    # remove stopwords\n",
    "    text = [x for x in text.split() if x not in stop]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pm_features(text):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features_low = dict()\n",
    "\n",
    "    # first clean text\n",
    "    text = clean_text(text)\n",
    "\n",
    "    \n",
    "    keyword_animal = ['mice', 'mouse', 'model', 'mammary', \n",
    "                      'rat','xenografts','dog','canie'\n",
    "                      'vivo','cycle','mutated','preclinical',\n",
    "                      'prostate','pten','liver','met','animal',\n",
    "                      'mgkg','human', 'xenograft']\n",
    "\n",
    "    keyword_human = ['gastrectomy','imatinib','gastric','stomach',\n",
    "                 'fgfr1','prognostic','mutation','gastrointestinal',\n",
    "                 'mutations','families','shorter','inhibitor',\n",
    "                 'kit','located','lethal','kras','dose',\n",
    "                 'tract','pfs','mutated']\n",
    "\n",
    "    keyword_not = ['transplantation','symptoms','female','male',\n",
    "                   'driver','pressure','pancreaticoduodenectomy',\n",
    "                   'surface','triple','women',\n",
    "                   'a549','mortality','adjuvant',\n",
    "                   'bypass','basis','myxoid']\n",
    "        \n",
    "    match_human = 0\n",
    "    match_animal = 0\n",
    "    match_not = 0\n",
    "    \n",
    "    for k in keyword_human:\n",
    "        match_human += text.count(k)\n",
    "        \n",
    "    for k in keyword_animal:\n",
    "        match_animal += text.count(k)\n",
    "            \n",
    "    for k in keyword_not:\n",
    "        match_not += text.count(k)\n",
    "\n",
    "    features_low['match_human'] = match_human\n",
    "    features_low['match_animal'] = match_animal\n",
    "    features_low['match_not'] = match_not\n",
    "    \n",
    "    return features_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gene_features(text, topic, query_topics_gene):\n",
    "    \n",
    "    # dictionary to save results\n",
    "    features_low = dict()\n",
    "    \n",
    "    query_genes = query_topics_gene[topic]\n",
    "    \n",
    "    # there might be several genes in the query\n",
    "    # we take the average for both low and mid level features\n",
    "    match_gene_list = []\n",
    "    has_variation_list = []\n",
    "    match_variation_list = []\n",
    "    match_total_variation_list = []\n",
    "    has_other_info_list = []\n",
    "    match_other_info_list = []\n",
    "    \n",
    "    differ_variant_list = []\n",
    "    exact_list = []\n",
    "    missing_gene_list = []\n",
    "    missing_variant_list = []\n",
    "    \n",
    "    for query_gene in query_genes.split(\",\"):\n",
    "        \n",
    "        query_gene = query_gene.strip()\n",
    "        genes,variant,other_info = parsing_query_gene(query_gene)\n",
    "        \n",
    "        # 1) match gene\n",
    "        match_gene = 0\n",
    "        for gene in genes:\n",
    "            if \" \" not in gene: \n",
    "                match_gene += text.count(gene)\n",
    "                aliases = get_gene_alias_and_variations(gene)[\"aliases\"]\n",
    "                for alias in aliases:\n",
    "                    match_gene += text.count(alias)\n",
    "            else: # special case: the gene is a phrase\n",
    "                for g in gene.split(\" \"):\n",
    "                    g = g.strip()\n",
    "                    match_gene += text.lower().count(g.lower())\n",
    "\n",
    "\n",
    "        # 2) match variation\n",
    "        if variant != 0:\n",
    "            has_variation,match_variation,match_total_variation = 1,0,0\n",
    "            # match variation\n",
    "            match_variation += text.count(variant)\n",
    "            # match all the variations\n",
    "            for gene in genes:\n",
    "                variants = get_gene_alias_and_variations(gene)[\"variants\"]\n",
    "                for v in variants:\n",
    "                    match_total_variation += text.count(v)\n",
    "        else:\n",
    "            has_variation,match_variation,match_total_variation = 0,0,0\n",
    "            # match all the variations\n",
    "            for gene in genes:\n",
    "                variants = get_gene_alias_and_variations(gene)[\"variants\"]\n",
    "                for v in variants:\n",
    "                    match_total_variation += text.count(v)\n",
    "\n",
    "\n",
    "        # 3) match other info\n",
    "        if other_info != 0:\n",
    "            has_other_info, match_other_info = 1,0\n",
    "            for o in other_info.split(\" \"):\n",
    "                o = o.strip()\n",
    "                match_other_info += text.lower().count(o.lower())\n",
    "        else:\n",
    "            has_other_info, match_other_info = 0,0\n",
    "            \n",
    "        match_gene_list.append(match_gene)\n",
    "        has_variation_list.append(has_variation)\n",
    "        match_variation_list.append(match_variation)\n",
    "        match_total_variation_list.append(match_total_variation)\n",
    "        has_other_info_list.append(has_other_info)\n",
    "        match_other_info_list.append(match_other_info)\n",
    "        \n",
    "        \n",
    "    # add the average to dictionary\n",
    "    features_low['match_gene'] = np.mean(match_gene_list)\n",
    "    features_low['has_variation'] = np.mean(has_variation_list)\n",
    "    features_low['match_variation'] = np.mean(match_variation_list)\n",
    "    features_low['match_total_variation'] = np.mean(match_total_variation_list)\n",
    "    features_low['has_other_info'] = np.mean(has_other_info_list)\n",
    "    features_low['match_other_info'] = np.mean(match_other_info_list)\n",
    "\n",
    "    return features_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_docid(docid):\n",
    "    folder_path = \"/Users/jiamingqu/Desktop/corpus/\"\n",
    "    lines = []\n",
    "    with open(folder_path+docid+\".txt\",'r') as f:\n",
    "            for line in f.readlines():\n",
    "                lines.append(line.strip())\n",
    "    full_text = \" \".join(lines)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ltr_low_features(year, from_retrieval=True):\n",
    "    \n",
    "    '''\n",
    "    Generate the low-level features for a year\n",
    "    '''\n",
    "    \n",
    "    # feature table\n",
    "    features=[ 'count_match_self', 'count_match_ancestor', 'count_match_descendant',\n",
    "               'age_missing_numeric', 'age_diff_numeric', 'age_missing_text',\n",
    "               'age_match_text', 'gender_missing', 'gender_diff',\n",
    "               'match_gene', 'has_variation', 'match_variation', \n",
    "               'match_total_variation', 'has_other_info', 'match_other_info',\n",
    "               'match_human','match_animal','match_not',\n",
    "               'topicid','docid','year','rel']\n",
    "    df_low = pd.DataFrame(columns=features)\n",
    "    \n",
    "    # read dataframe: from retrieval results or from qrel\n",
    "    if from_retrieval==True:\n",
    "        df = pd.read_csv(\"../../scripts/searching/2017.searching/2017.basic.query.result.txt\",sep=\"\\t\")\n",
    "        df.columns = ['topicid', 'q0', 'docid', 'rank', 'score', 'run_name', 'title', 'content']\n",
    "        df['topicid'] = df['topicid'].astype(int)\n",
    "        qrel = pd.read_csv(\"../../data/topics/2017qrel.txt\", sep = \" \", header=None)\n",
    "        qrel.columns = ['topicid', 'q0', 'docid', 'Relevance']\n",
    "        df = df.merge(qrel, on=['topicid','docid'], how='left')\n",
    "        df.fillna(0,inplace=True)\n",
    "        \n",
    "    else:\n",
    "        df =pd.read_csv(\"../../data/parsedjudgements/judgments\"+str(year)+\".csv\")\n",
    "        df['topicid'] = df['topicid'].astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # load resources and query topics\n",
    "    # disease\n",
    "    query_topics_disease = read_query_topics(year,\"disease\")\n",
    "    disease_expansion_terms = dict()\n",
    "    with open (\"../../scripts/classifier/disease/\" + str(year) + \".disease.expansion.json\", 'r') as f:\n",
    "        for data in f:\n",
    "            disease_expansion_terms = json.loads(data)\n",
    "    f.close()\n",
    "    # read acronyms: a dict of <disease, acronyms>\n",
    "    acronyms_dict = dict()\n",
    "    with open(\"../../scripts/classifier/disease/acronyms.json\",'r') as f:\n",
    "         for line in f.readlines():\n",
    "            acronyms_dict = json.loads(line)\n",
    "    f.close()\n",
    "    \n",
    "    # demo\n",
    "    query_topics_demo = read_query_topics(year,\"demo\")\n",
    "    \n",
    "    # gene\n",
    "    query_topics_gene = read_query_topics(year,\"gene\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # loop over topics\n",
    "    for topic in set(df.topicid):\n",
    "\n",
    "        df_topic = df.loc[df.topicid==topic]\n",
    "        print(\"Parsing Topic No.{}\".format(topic))\n",
    "        \n",
    "        for index,rows in df_topic.iterrows():\n",
    "            \n",
    "            rel = df_topic.loc[index,\"Relevance\"]\n",
    "            \n",
    "            docid = str(df_topic.loc[index,\"docid\"])\n",
    "            if from_retrieval == True:\n",
    "                text = df_topic.loc[index,'title'] + \" \" + df_topic.loc[index,'content']\n",
    "            else:\n",
    "                text = read_text_docid(docid)\n",
    "                \n",
    "            disease_features = generate_disease_features(text,topic,query_topics_disease,disease_expansion_terms,acronyms_dict)\n",
    "            gene_features = generate_gene_features(text,topic,query_topics_gene)\n",
    "            demo_features = generate_demo_features(text,topic,query_topics_demo)\n",
    "            pm_features = generate_pm_features(text)\n",
    "            \n",
    "            record_low = dict(**disease_features, **gene_features, **demo_features, **pm_features)\n",
    "            record_low['topicid'] = topic\n",
    "            record_low['docid'] = docid\n",
    "            record_low['year'] = year\n",
    "            record_low['rel'] = rel\n",
    "            \n",
    "            # add to feature table\n",
    "            df_low = df_low.append(record_low, ignore_index=True)\n",
    "\n",
    "    assert df_low.shape[0] == df.shape[0]\n",
    "    print(df_low.shape)\n",
    "    \n",
    "    if from_retrieval == True:\n",
    "        df_low.to_csv(str(year)+\".low.features.from.retrieval.csv\", sep=\",\", index=False)\n",
    "    else:\n",
    "        df_low.to_csv(str(year)+\".low.features.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_ltr_low_features(2017, from_retrieval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
