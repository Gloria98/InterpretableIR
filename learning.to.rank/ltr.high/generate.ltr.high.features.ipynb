{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training features from 2017 predicted probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_prob_qrel = pd.read_csv(\"2017.high.txt\", sep=\" \", header=None)\n",
    "col_names = ['rel', 'topicid', \"Human_PM\", \"Animal_PM\", \"Not_PM\", \\\n",
    "\"Disease_Exact\", \"Disease_General\",\"Disease_Specific\", \"Disease_Not\",\\\n",
    "\"Gene_Exact\", \"Gene_Missing\", \"Gene_Missing_Variant\",\"Gene_Diff_Variant\", \\\n",
    "\"Demo_Match\", \"Demo_Notdiscussed\", \"Demo_Exclude\" ,'none', 'docid']\n",
    "high_prob_qrel.columns = col_names\n",
    "del high_prob_qrel['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use retrieved results\n",
    "retrieval_results_2017 = pd.read_csv(\"../../scripts/searching/2017.searching/2017.basic.query.result.txt\",sep=\"\\t\")\n",
    "retrieval_results_2017=retrieval_results_2017[['TOPIC_NO','ID', 'SCORE']]\n",
    "retrieval_results_2017.columns = ['topicid', 'docid', 'score']\n",
    "\n",
    "retrieval_results_2017 = retrieval_results_2017.merge(high_prob_qrel, on = ['topicid', 'docid'], how='left')\n",
    "retrieval_results_2017.dropna(inplace=True)\n",
    "retrieval_results_2017.rel = retrieval_results_2017.rel.astype(int)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "retrieval_results_2017[['score']]=scaler.fit_transform(retrieval_results_2017[['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../../training.for.2018.txt\", 'w') as f:\n",
    "    \n",
    "    for i in set(retrieval_results_2017.topicid):\n",
    "        \n",
    "        df_topic = retrieval_results_2017.loc[retrieval_results_2017.topicid == i]\n",
    "        \n",
    "        for index, rows in df_topic.iterrows():\n",
    "            \n",
    "            topicid = rows[\"topicid\"]\n",
    "            docid = rows[\"docid\"]\n",
    "            rel = rows[\"rel\"]\n",
    "\n",
    "            records = rows.to_dict()\n",
    "            line = \"{} qid:{} {} {} {} {} {} {} {} {} {} {} {} {} {} {} 15:{} #{}\".format(rel, topicid, \\\n",
    "                  records[\"Human_PM\"], records[\"Animal_PM\"], records[\"Not_PM\"], \\\n",
    "                  records[\"Disease_Exact\"], records[\"Disease_General\"],records[\"Disease_Specific\"], records[\"Disease_Not\"],\\\n",
    "                  records[\"Gene_Exact\"], records[\"Gene_Missing\"], records[\"Gene_Missing_Variant\"],records[\"Gene_Diff_Variant\"], \\\n",
    "                  records[\"Demo_Match\"], records[\"Demo_Notdiscussed\"], records[\"Demo_Exclude\"], records['score'], \\\n",
    "                  docid)\n",
    "\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate testing features from 2018 predicted probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_prob_2018 = pd.read_csv(\"../../scripts/reranking/2018.high.features.csv\")\n",
    "retrieval_results_2018 = pd.read_csv(\"../../scripts/searching/2018.searching/2018.basic.query.result.txt\",sep=\"\\t\")\n",
    "high_prob_2018 = high_prob_2018.merge(retrieval_results_2018, left_on=['topicid', 'docid'], right_on=['TOPIC_NO', 'ID'], how='left')\n",
    "high_prob_2018[['SCORE']] = scaler.fit_transform(high_prob_2018[['SCORE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in 2018 retrieval result, there is no relevance score\n",
    "# we add relevance score to evaluate in Ranklib\n",
    "qrel_2018 = pd.read_csv(\"../../data/topics/2018qrel.txt\",sep=\" \",header=None)\n",
    "qrel_2018.columns = ['topicid','q0','docid','rel']\n",
    "qrel_2018 = qrel_2018[['topicid','docid','rel']]\n",
    "\n",
    "# merge and fill na\n",
    "high_prob_2018['topicid'] = high_prob_2018['topicid'].astype(int)\n",
    "high_prob_2018['docid'] = high_prob_2018['docid'].astype(str)\n",
    "qrel_2018['topicid'] = qrel_2018['topicid'].astype(int)\n",
    "qrel_2018['docid'] = qrel_2018['docid'].astype(str)\n",
    "high_prob_2018 = high_prob_2018.merge(qrel_2018, on = [\"topicid\",\"docid\"],how=\"left\")\n",
    "high_prob_2018.fillna(0, inplace=True)\n",
    "high_prob_2018['rel'] = high_prob_2018['rel'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../../testing.for.2018.txt\", 'w') as f:\n",
    "    \n",
    "    for i in set(high_prob_2018.topicid):\n",
    "        \n",
    "        df_topic = high_prob_2018.loc[high_prob_2018.topicid == i]\n",
    "        \n",
    "        for index, rows in df_topic.iterrows():\n",
    "            \n",
    "            topicid = rows[\"topicid\"]\n",
    "            docid = rows[\"docid\"]\n",
    "            rel = rows[\"rel\"]\n",
    "\n",
    "            records = rows.to_dict()\n",
    "            line = \"{} qid:{} 1:{} 2:{} 3:{} 4:{} 5:{} 6:{} 7:{} 8:{} 9:{} 10:{} 11:{} 12:{} 13:{} 14:{} 15:{} #{}\".format(rel, topicid, \\\n",
    "                  records[\"Human_PM\"], records[\"Animal_PM\"], records[\"Not_PM\"], \\\n",
    "                  records[\"Disease_Exact\"], records[\"Disease_General\"],records[\"Disease_Specific\"], records[\"Disease_Not\"],\\\n",
    "                  records[\"Gene_Exact\"], records[\"Gene_Missing\"], records[\"Gene_Missing_Variant\"],records[\"Gene_Diff_Variant\"], \\\n",
    "                  records[\"Demo_Match\"], records[\"Demo_Notdiscussed\"], records[\"Demo_Exclude\"],records['SCORE'],\\\n",
    "                  docid)\n",
    "\n",
    "            f.write(line + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
